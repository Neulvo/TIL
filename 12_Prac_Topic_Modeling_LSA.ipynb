{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12 Prac. Topic Modeling LSA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrwgjvm5rj7NHCte6EtvDG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neulvo/TIL/blob/master/12_Prac_Topic_Modeling_LSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfTi_soweK_w",
        "colab_type": "text"
      },
      "source": [
        "#토픽 모델링 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEyOiMzKeP_U",
        "colab_type": "text"
      },
      "source": [
        "## 0 환경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAJukjffeRR7",
        "colab_type": "text"
      },
      "source": [
        "## 1. 잠재의미 분석 (Latent Semantic Analysis LSA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcjmKLdWeUz5",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 직접 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2YtTYjFeWYd",
        "colab_type": "text"
      },
      "source": [
        "1) 직접 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3orA4ECQeXwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import randomized_svd\n",
        "\n",
        "class LSA:\n",
        "    def __init__(self, doc_ls, topic_num):\n",
        "      self.doc_ls = doc_ls\n",
        "      self.topic_num = topic_num\n",
        "\n",
        "      self.term2idx, self.idx2term = self.toIdxDict( ' '.join(doc_ls).split())\n",
        "\n",
        "      self.doc2idx, self.idx2doc = self.toIdxDict(doc_ls)\n",
        "\n",
        "      self.tdm  = self.TDM(doc_ls)\n",
        "      self.U, self.s, self.VT = self.SVD(self.tdm)\n",
        "      \n",
        "      self.term_mat = self.TermVectorMatrix(self.U, self.s, topic_num)\n",
        "      self.doc_mat = self.DocVectorMatrix(self.s, self.VT, topic_num)\n",
        "\n",
        "      self.term_doc_mat = self.TermDocVectorMatrix(self.U, self.s, self.VT, topic_num)\n",
        "\n",
        "    # 리스트내 값을 index로 변환하는 dict과 \n",
        "    # index를 리스트내 값으로 변환하는 dict\n",
        "    def toIdxDict(self, ls):\n",
        "      any2idx = defaultdict(lambda : len(any2idx))\n",
        "      idx2any = defaultdict()\n",
        "\n",
        "      for item in ls:\n",
        "        any2idx[item]\n",
        "        idx2any[any2idx[item]] = item\n",
        "\n",
        "      return any2idx, idx2any\n",
        "\n",
        "    \n",
        "    def TDM(self, doc_ls):\n",
        "      tdm = np.zeros([len(self.term2idx), len(doc_ls)])\n",
        "      for doc_idx, doc in enumerate(doc_ls):\n",
        "        for term in doc.split():\n",
        "          tdm[self.term2idx[term], doc_idx] += 1\n",
        "\n",
        "      return tdm\n",
        "        \n",
        "    \n",
        "    # 특이값 분해\n",
        "    def SVD(self, tdm):\n",
        "      U, s, VT = randomized_svd(tdm, n_components = 6, n_iter=20, random_state=None)\n",
        "      return U, s, VT\n",
        "\n",
        "\n",
        "    # 토픽별 주요 키워드 출력\n",
        "    def TopicModeling(self, topic_num = 3):\n",
        "      for i in range(topic_num):\n",
        "        score = self.U[:, i:i+1].T\n",
        "        sorted_index = np.argsort(-score)\n",
        "\n",
        "        a = []\n",
        "        for j in sorted_index[0, : topic_num]:\n",
        "          a.append((self.idx2term[j], score[0, j].round(3)))\n",
        "\n",
        "        print(\"Topic {} - {}\".format(i + 1, a))\n",
        "\n",
        "    \n",
        "    def TermDocVectorMatrix(self, u, s, vt, topic_num):\n",
        "      term_doc_mat = np.matrix(u[:, :topic_num]) * np.diag(s[:topic_num]) * np.matrix(vt[:topic_num, :])\n",
        "      return term_doc_mat  \n",
        "\n",
        "    # 키워드를 입력했을 때 단어 벡터 반환\n",
        "    def GetTermVector(self, term):\n",
        "      vec = self.term_mat[self.term2idx[term]]\n",
        "      print(\"{} = {}\".format(term, vec))\n",
        "      return vec\n",
        "        \n",
        "            \n",
        "    # 문서를 입력했을 때 문서 벡터 반환\n",
        "    def GetDocVector(self, doc):\n",
        "      vec = self.doc_mat[self.doc2idx[doc]]\n",
        "      print(\"{} = {}\".format(doc, vec))\n",
        "      return vec\n",
        "    \n",
        "    \n",
        "    def TermVectorMatrix(self, u, s, topic_num):\n",
        "      term_mat = np.matrix(u[:, :topic_num]) * np.diag(s[:topic_num]) \n",
        "      return term_mat\n",
        "        \n",
        "    def DocVectorMatrix(self, s, vt, topic_num):\n",
        "      doc_mat = np.diag(s[:topic_num])  * np.matrix(vt[:topic_num, :])\n",
        "      return doc_mat.T\n",
        "        \n",
        "    def GetTermSimilarity(self, term1, term2):\n",
        "      sim = cosine_similarity(self.GetTermVector(term1), self.GetTermVector(term2))\n",
        "      print('({},{}) term similarity = {}'.format(term1, term2, sim[0][0]))\n",
        "      return sim\n",
        "        \n",
        "    def GetDocSimilarity(self, doc1, doc2):\n",
        "      sim  = cosine_similarity(self.GetDocVector(doc1), self.GetDocVector(doc2))\n",
        "      print('({},{}) doc similarity = {}'.format(doc1, doc2, sim[0][0]))\n",
        "      return sim\n",
        "    \n",
        "    def Compression(self, round_num=0):        \n",
        "      print(self.tdm)\n",
        "      print(self.term_doc_mat.round(round_num))\n",
        "        \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doe4KZM2g6Cs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fdb047f-8862-4446-a78d-0e0dcda6fae0"
      },
      "source": [
        "doc_ls = ['바나나 사과 포도 포도 짜장면',\n",
        "         '사과 포도',\n",
        "         '포도 바나나',\n",
        "         '짜장면 짬뽕 탕수육',\n",
        "         '볶음밥 탕수육',\n",
        "         '짜장면 짬뽕',\n",
        "         '라면 스시',\n",
        "         '스시 짜장면',\n",
        "         '가츠동 스시 소바',\n",
        "         '된장찌개 김치찌개 김치',\n",
        "         '김치 된장 짜장면',\n",
        "         '비빔밥 김치'\n",
        "         ]\n",
        "\n",
        "lsa = LSA(doc_ls,3)\n",
        "X=lsa.TDM(doc_ls)\n",
        "print(X)\n",
        "print('== 토픽 모델링 ==')\n",
        "lsa.TopicModeling(3)\n",
        "\n",
        "print('\\n== 단어 벡터 ==')\n",
        "lsa.GetTermVector('사과')\n",
        "lsa.GetTermVector('포도')\n",
        "print('\\n== 단어 유사도 ==')\n",
        "lsa.GetTermSimilarity('사과','바나나')\n",
        "lsa.GetTermSimilarity('사과','짜장면')\n",
        "lsa.GetTermSimilarity('포도','짜장면')\n",
        "lsa.GetTermSimilarity('사과','스시')\n",
        "print('\\n== 문서 벡터 ==')\n",
        "lsa.GetDocVector('사과 포도')\n",
        "lsa.GetDocVector('짜장면 짬뽕')\n",
        "print('\\n== 문서 유사도 ==')\n",
        "lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n",
        "lsa.GetDocSimilarity('사과 포도', '바나나 사과 포도 포도 짜장면')\n",
        "print('\\n== 토픽 차원수로 압축 ==')\n",
        "lsa.Compression(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "== 토픽 모델링 ==\n",
            "Topic 1 - [('포도', 0.697), ('짜장면', 0.486), ('바나나', 0.348)]\n",
            "Topic 2 - [('짜장면', 0.584), ('짬뽕', 0.356), ('김치', 0.337)]\n",
            "Topic 3 - [('김치', 0.611), ('된장찌개', 0.264), ('김치찌개', 0.264)]\n",
            "\n",
            "== 단어 벡터 ==\n",
            "사과 = [[ 1.1233207  -0.47387139  0.03306518]]\n",
            "포도 = [[ 2.24664139 -0.94774279  0.06613036]]\n",
            "\n",
            "== 단어 유사도 ==\n",
            "사과 = [[ 1.1233207  -0.47387139  0.03306518]]\n",
            "바나나 = [[ 1.1233207  -0.47387139  0.03306518]]\n",
            "(사과,바나나) term similarity = 1.0\n",
            "사과 = [[ 1.1233207  -0.47387139  0.03306518]]\n",
            "짜장면 = [[ 1.56565532  1.42901928 -0.15505762]]\n",
            "(사과,짜장면) term similarity = 0.41525477232165237\n",
            "포도 = [[ 2.24664139 -0.94774279  0.06613036]]\n",
            "짜장면 = [[ 1.56565532  1.42901928 -0.15505762]]\n",
            "(포도,짜장면) term similarity = 0.41525477232165253\n",
            "사과 = [[ 1.1233207  -0.47387139  0.03306518]]\n",
            "스시 = [[ 0.22210874  0.6262725  -1.0851879 ]]\n",
            "(사과,스시) term similarity = -0.05358134239633554\n",
            "\n",
            "== 문서 벡터 ==\n",
            "사과 포도 = [[ 1.04529381 -0.58112882  0.04777778]]\n",
            "짜장면 짬뽕 = [[ 0.61011838  0.93971158 -0.17760018]]\n",
            "\n",
            "== 문서 유사도 ==\n",
            "사과 포도 = [[ 1.04529381 -0.58112882  0.04777778]]\n",
            "포도 바나나 = [[ 1.04529381 -0.58112882  0.04777778]]\n",
            "(사과 포도,포도 바나나) doc similarity = 1.0\n",
            "사과 포도 = [[ 1.04529381 -0.58112882  0.04777778]]\n",
            "바나나 사과 포도 포도 짜장면 = [[ 2.57622211 -0.57810176  0.02087167]]\n",
            "(사과 포도,바나나 사과 포도 포도 짜장면) doc similarity = 0.9587158948422938\n",
            "\n",
            "== 토픽 차원수로 압축 ==\n",
            "[[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "[[ 1.  0.  0.  0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
            " [ 1.  0.  0.  0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
            " [ 2.  1.  1.  0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
            " [ 1.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.]\n",
            " [ 0. -0. -0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.  0.  0.  0.  0. -0.  0. -0.]\n",
            " [-0. -0. -0.  0.  0.  0.  0.  0.  0. -0. -0. -0.]\n",
            " [ 0. -0. -0.  0.  0.  0.  0.  1.  1. -0. -0. -0.]\n",
            " [-0. -0. -0.  0.  0.  0.  0.  0.  0. -0. -0. -0.]\n",
            " [-0. -0. -0.  0.  0.  0.  0.  0.  0. -0. -0. -0.]\n",
            " [-0. -0. -0.  0. -0.  0. -0. -0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0. -0.  0. -0. -0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.  0. -0. -0. -0.  1.  1.  1.]\n",
            " [ 0. -0. -0.  0.  0.  0. -0.  0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0. -0.  0. -0. -0. -0.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiUjS6Nrlkmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}